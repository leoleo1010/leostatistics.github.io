<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The central limit problem</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }

        header {
            background-color: #007acc;
            color: #fff;
            padding: 1em;
            text-align: center;
        }

        main {
            display: flex;
            max-width: 800px;
            margin: 20px auto;
            border:groove;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }

        img {
            max-width: 100%;
            height: 40%;
            margin-top: 20%;
            margin-right: 1%;
        }
        figure{
            max-width: 30%;
            height: 40%;
            margin-top: 10%;
            margin-right: 1%; 
        
        }

        .content {
            flex: 1;
            padding: 20px;
        }

        h1 {
            color: #007acc;
        }

        p {
            line-height: 1.6;
        }
    </style>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
</head>
<body>
    <header>
        <h1 style="color: white;">CLT: The central limit problem</h1>
    </header>
    <main>
        <div class="content">
            <h2> Meaning </h2>
            <p>In probability theory, the central limit theorem (CLT) states that, under appropriate conditions, the distribution of a normalized version of the sample mean converges to a standard normal distribution. This holds even if the original variables themselves are not normally distributed.</p>
            <p>The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.</p>
            <p>An elementary form of the theorem states the following. Let \(X_{1},X_{2},\dots ,X_{n}\) denote a random sample of \(n\) independent observations from a population with overall expected value (average)
                \(μ\)  and finite variance \(σ ^{2}\), and let \({\bar {X}}_{n}\) denote the sample mean of that sample (which is itself a random variable). 
                Then the limit as \(n\to \infty\)  of the distribution of \({\displaystyle {\frac{{\bar {X}}_{n}-\mu }{\sigma _{{\bar {X}}_{n}}}},}\)
                where \({\displaystyle \sigma _{{\bar {X}}_{n}}={\frac {\sigma }{\sqrt {n}}},}\) is the standard normal distribution.</p>
            <p>
                In other words, suppose that a large sample of observations is obtained, each observation being randomly produced in a way that does not depend on the values of the other observations, and that the average (arithmetic mean) of the observed values is computed. If this procedure is performed many times, resulting in a collection of observed averages, the central limit theorem says that if the sample size was large enough, the probability distribution of these averages will closely approximate a normal distribution.
            </p>
        </div>
        <figure>
            <img src="adm.PNG" alt="Immagine correlata alla ricerca">
            <figcaption> The central limit theorem was first established within the framework of binomial distribution by <b>Abraham de Moivre</b> </figcaption>
            <img src="psl.PNG" alt="Immagine correlata alla ricerca">
            <figcaption> <b>Pierre Simon Laplace</b> formulated the proof of the theorem.</figcaption>
        </figure>
    </main>
    <main>
        <div class="content">
            <h4> Classical CLT </h4>
            <p>
                Let \({\displaystyle \{X_{1},\ldots ,X_{n}}\}\) be a sequence of i.i.d. random variables having a distribution with expected value given by 
                \(μ\) and finite variance given by \({\displaystyle σ ^{2}.}\)Suppose we are interested in the sample average
                $${\displaystyle {\bar {X}}_{n}\equiv {\frac {X_{1}+\cdots +X_{n}}{n}}.}$$ 
                By the law of large numbers, the sample average converges almost surely (and therefore also converges in probability) to the expected value \(μ\)  as \(n\to \infty\)
            </p>
            <p>
            The classical central limit theorem describes the size and the distributional form of the stochastic fluctuations around the deterministic number \(μ\)  during this convergence. More precisely, it states that as n gets larger, the distribution of the difference between the sample average 
            \({\displaystyle {\bar {X}}_{n}}\) and its limit \(μ\),  when multiplied by the factor 
            \({\sqrt {n}}\) (that is, \({\displaystyle {\sqrt {n}}({\bar {X}}_{n}-\mu )}\)) approximates the normal distribution with mean 0
            and variance \({\displaystyle σ^{2}.}\) For large enough n, the distribution of \({\displaystyle {\bar {X}}_{n}}\) gets arbitrarily close to the normal distribution with mean
            \(μ\) and variance \({\displaystyle σ ^{2}/n.}\) 
            </p>
            <p>
                The usefulness of the theorem is that the distribution of 
            \({\displaystyle {\sqrt {n}}({\bar {X}}_{n}-\mu )}\) approaches normality regardless of the shape of the distribution of the individual 
            \({\displaystyle X_{i}.}\) Formally, the theorem can be stated as follows:
            </p>

            <p>
                In the case \({\displaystyle \sigma >0,}\) convergence in distribution means that the cumulative distribution functions of 
                \({\displaystyle {\sqrt {n}}({\bar {X}}_{n}-\mu )}\) converge pointwise to the cdf of the 
                \({\mathcal {N}}(0,\sigma ^{2})\) distribution: for every real number \(z\),
                $${\displaystyle \lim _{n\to \infty }\mathbb {P} \left[{\sqrt {n}}({\bar {X}}_{n}-\mu )\leq z\right]=\lim _{n\to \infty }\mathbb {P} \left[{\frac {{\sqrt {n}}({\bar {X}}_{n}-\mu )}{\sigma }}\leq {\frac {z}{\sigma }}\right]=\Phi \left({\frac {z}{\sigma }}\right),}$$
                where \(Φ (z)\) is the standard normal cdf evaluated at \(z\). The convergence is uniform in \(z\) in the sense that
                $${\displaystyle \lim _{n\to \infty }\;\sup _{z\in \mathbb {R} }\;\left|\mathbb {P} \left[{\sqrt {n}}({\bar {X}}_{n}-\mu )\leq z\right]-\Phi \left({\frac {z}{\sigma }}\right)\right|=0~,}$$
                where  \(sup\)  denotes the least upper bound (or supremum) of the set.
            </p>
        </div>
    </main>
    <main>
        <div class="content">
            <h2> Proof</h2>
            <p>Assume 
                \({\textstyle \{X_{1},\ldots ,X_{n},\ldots \}}\) are independent and identically distributed random variables, each with mean 
                \({\textstyle \mu }\) and finite variance \({\textstyle \sigma ^{2}}\).
                The sum \({\textstyle X_{1}+\cdots +X_{n}}\) has mean \({\textstyle n\mu }\) and variance \({\textstyle n\sigma ^{2}}\). Consider the random variable
                $${\displaystyle Z_{n}={\frac {X_{1}+\cdots +X_{n}-n\mu }{\sqrt {n\sigma ^{2}}}}=\sum _{i=1}^{n}{\frac {X_{i}-\mu }{\sqrt {n\sigma ^{2}}}}=\sum _{i=1}^{n}{\frac {1}{\sqrt {n}}}Y_{i},}$$
                where in the last step we defined the new random variables \({\textstyle Y_{i}={\frac {X_{i}-\mu }{\sigma }}}\), each with zero mean and unit variance 
                (\({\textstyle \operatorname {var} (Y)=1})\). The characteristic function of \({\textstyle Z_{n}}\) is given by
                $${\displaystyle \varphi _{Z_{n}}\!(t)=\varphi _{\sum _{i=1}^{n}{{\frac {1}{\sqrt {n}}}Y_{i}}}\!(t)\ =\ \varphi _{Y_{1}}\!\!\left({\frac {t}{\sqrt {n}}}\right)\varphi _{Y_{2}}\!\!\left({\frac {t}{\sqrt {n}}}\right)\cdots \varphi _{Y_{n}}\!\!\left({\frac {t}{\sqrt {n}}}\right)\ =\ \left[\varphi _{Y_{1}}\!\!\left({\frac {t}{\sqrt {n}}}\right)\right]^{n},}$$
                where in the last step we used the fact that all of the \({\textstyle Y_{i}}\) are identically distributed. The characteristic function of 
                \({\textstyle Y_{1}}\) is, by Taylor&apos;s theorem,
                $${\displaystyle \varphi _{Y_{1}}\!\left({\frac {t}{\sqrt {n}}}\right)=1-{\frac {t^{2}}{2n}}+o\!\left({\frac {t^{2}}{n}}\right),\quad \left({\frac {t}{\sqrt {n}}}\right)\to 0}$$
                where \({\textstyle o(t^{2}/n)}\) is &quot;little o notation&quot; for some function of \({\textstyle t}\) that goes to zero more rapidly than \({\textstyle t^{2}/n}\).
                By the limit of the exponential function (\({\textstyle e^{x}=\lim _{n\to \infty }\left(1+{\frac {x}{n}}\right)^{n}})\), the characteristic function of 
                \(Z_{n}\) equals
                $${\displaystyle \varphi _{Z_{n}}(t)=\left(1-{\frac {t^{2}}{2n}}+o\left({\frac {t^{2}}{n}}\right)\right)^{n}\rightarrow e^{-{\frac {1}{2}}t^{2}},\quad n\to \infty .}$$
                All of the higher order terms vanish in the limit \({\textstyle n\to \infty }\). 
            </p>
            <p>The right hand side equals the characteristic function of a standard normal distribution 
                \({\textstyle {\mathcal {N}}(0,1)}\), which implies through Lévy&apos;s continuity theorem that the distribution of \({\textstyle Z_{n}}\) will approach 
                \({\textstyle {\mathcal {N}}(0,1)}\) as \({\textstyle n\to \infty }\). Therefore, the sample average
                $${\displaystyle {\bar {X}}_{n}={\frac {X_{1}+\cdots +X_{n}}{n}}}$$ is such that 
                $${\displaystyle {\frac {\sqrt {n}}{\sigma }}({\bar {X}}_{n}-\mu )=Z_{n}}$$
                converges to the normal distribution \({\textstyle {\mathcal {N}}(0,1)}\), from which the central limit theorem follows.
            </p>

        </div>
    </main>
    <main>
        <div class="content">
            <h2> Simulation </h2>
            <p>
                A simple example of the central limit theorem is rolling many identical, unbiased dice. The distribution of the sum (or average) of the rolled numbers will be well approximated by a normal distribution. Since real-world quantities are often the balanced sum of many unobserved random events, the central limit theorem also provides a partial explanation for the prevalence of the normal probability distribution. It also justifies the approximation of large-sample statistics to the normal distribution in controlled experiments.

            </p>
            <img src="pr.PNG" style=" max-width: 100%;
            height: 70%;
            margin-top: 0%;
            margin-right: 0%;" alt="Immagine correlata alla ricerca">
        </div>
    </main>
    
</body>
</html>